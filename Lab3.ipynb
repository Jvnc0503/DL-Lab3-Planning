{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engine"
      ],
      "metadata": {
        "id": "XUUrppw9sSwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wJWxfB9BRCw8"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade transformers accelerate bitsandbytes einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer"
      ],
      "metadata": {
        "id": "mC-oILGCqu73"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BOfBPu9CRLGP"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"Qwen/Qwen3-8B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563,
          "referenced_widgets": [
            "61284b4523404474bf6a6a65fedaf8f3",
            "288b63bbdd23400e8f1114d1335fa707",
            "7ff0a368d6564b4ca93f2a1e04fd8656",
            "494827445a8343859d82714dfe5bdea6",
            "88419be370434fcb91c9871dd65184ec",
            "1302a0ff92124f80964f2898444cf336",
            "95c31a75b7f541ec86386cdda63a70ac",
            "cd08466289694241a448ec5db68ed70d",
            "8d4578f6549f45e2ac6020e1af149fe5",
            "e78fa926798941468620aaf74461c8c6",
            "6e7eadc1f77646a6a1bed5e2230871cc"
          ]
        },
        "id": "grh8i3rhRNuf",
        "outputId": "9345f1a3-dbe7-48bb-bf46-fb1dc547ac6d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/399 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61284b4523404474bf6a6a65fedaf8f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen3ForCausalLM(\n",
              "  (model): Qwen3Model(\n",
              "    (embed_tokens): Embedding(151936, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-35): 36 x Qwen3DecoderLayer(\n",
              "        (self_attn): Qwen3Attention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "        )\n",
              "        (mlp): Qwen3MLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
              "    (rotary_emb): Qwen3RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Configuración de cuantización\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    use_fast=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "#                                  -------------------------------\n",
        "#           ---------             | --------                  --- |            -----------\n",
        "# texto -> |Tokenizer| - > IDs -> ||Codebook| -> Vectores -> |LLM|| -> IDs -> |unTokenizer| -> Texto (output)\n",
        "#           ---------             | --------                  --- |            -----------\n",
        "#                                  -------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bzJtofngRRyh"
      },
      "outputs": [],
      "source": [
        "def qwen( prompt: str,\n",
        "          system: str = \"You are a logical planning assitant\",\n",
        "          max_new_tokens: int=512) -> str:\n",
        "\n",
        "    # Alta temperatura: Creativo (alucinar)\n",
        "    #                   Las probabilidades seran homogeneas\n",
        "    # Menor temperatura: Más determinista\n",
        "    #                    Se aproxima a un one-hot\n",
        "\n",
        "    # enable_thinking: Habilita el modo de pensamiento\n",
        "    # do_sample: Muestre aleatorios entre tokens más probables\n",
        "    # top-p: Los token más probables hasta tener p de probabilidad\n",
        "    # top-k: Seleccionamos los k tokens mas probables\n",
        "    # stream: Controla si los tokens se van generando en tiempo real\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    # ID de tokens (respuesta)\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=False,\n",
        "        enable_thinking=False\n",
        "    )\n",
        "    # Traducimos los Ids como texto\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Parámetros estrictos para tareas lógicas\n",
        "    gen_kwargs = dict(\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,      # Apagar muestreo aleatorio para respuestas deterministas\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # resp = [input, output]\n",
        "        out = model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "    # Sólo la parte nueva:\n",
        "    gen_ids = out[0, inputs.input_ids.shape[1]:]\n",
        "    return tokenizer.decode(gen_ids, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluator"
      ],
      "metadata": {
        "id": "lBAlXr-hsZap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def limpiar_accion(accion_texto):\n",
        "    texto = accion_texto.replace('(', '').replace(')', '')\n",
        "    return texto.strip().lower()\n",
        "\n",
        "def calcular_score_plan(plan_generado, plan_optimo):\n",
        "    P = [limpiar_accion(p) for p in plan_generado if p.strip()]\n",
        "    G = [limpiar_accion(p) for p in plan_optimo if p.strip()]\n",
        "\n",
        "    L_P = len(P)\n",
        "    L_G = len(G)\n",
        "\n",
        "    if L_P == 0:\n",
        "        return 0.0\n",
        "\n",
        "    score_horizonte = 2.0 if L_P == L_G else 0.0\n",
        "\n",
        "    l_match = 0\n",
        "    for p_accion, g_accion in zip(P, G):\n",
        "        if p_accion == g_accion:\n",
        "            l_match += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    score_progreso = 3.0 * (l_match / L_G)\n",
        "    score_exacto = 5.0 if (l_match == L_G and L_P == L_G) else 0.0\n",
        "\n",
        "    return round(score_horizonte + score_progreso + score_exacto, 2)"
      ],
      "metadata": {
        "id": "0dN5qW_HsbQP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student Agent"
      ],
      "metadata": {
        "id": "qUK92YZfsoQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import deque\n",
        "\n",
        "class AssemblyAgent:\n",
        "    def __init__(self):\n",
        "        self.system_prompt = (\n",
        "            \"You are an expert deterministic planner for a toy planning domain. \"\n",
        "            \"Return only valid plan actions in canonical PDDL-like format, one per line.\"\n",
        "        )\n",
        "        self._patterns = [\n",
        "            (\n",
        "                re.compile(r\"\\battack\\s+(?:object\\s+)?([a-z])\\b\", re.IGNORECASE),\n",
        "                lambda m: f\"(attack {m.group(1).lower()})\",\n",
        "            ),\n",
        "            (\n",
        "                re.compile(r\"\\bsuccumb\\s+(?:object\\s+)?([a-z])\\b\", re.IGNORECASE),\n",
        "                lambda m: f\"(succumb {m.group(1).lower()})\",\n",
        "            ),\n",
        "            (\n",
        "                re.compile(\n",
        "                    r\"\\bfeast\\s+(?:object\\s+)?([a-z])\\s+(?:from\\s+(?:object\\s+)?)?([a-z])\\b\",\n",
        "                    re.IGNORECASE,\n",
        "                ),\n",
        "                lambda m: f\"(feast {m.group(1).lower()} {m.group(2).lower()})\",\n",
        "            ),\n",
        "            (\n",
        "                re.compile(\n",
        "                    r\"\\bovercome\\s+(?:object\\s+)?([a-z])\\s+(?:from\\s+(?:object\\s+)?)?([a-z])\\b\",\n",
        "                    re.IGNORECASE,\n",
        "                ),\n",
        "                lambda m: f\"(overcome {m.group(1).lower()} {m.group(2).lower()})\",\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "    def _split_facts(self, text: str) -> list[str]:\n",
        "        cleaned = text.strip().lower().rstrip(\".\")\n",
        "        parts = re.split(r\",|\\band\\b\", cleaned)\n",
        "        return [part.strip() for part in parts if part.strip()]\n",
        "\n",
        "    def _extract_final_statement(self, scenario_context: str) -> str:\n",
        "        parts = scenario_context.split(\"[STATEMENT]\")\n",
        "        return parts[-1] if len(parts) > 1 else scenario_context\n",
        "\n",
        "    def _parse_problem(self, scenario_context: str):\n",
        "        statement = self._extract_final_statement(scenario_context)\n",
        "        lines = [line.strip() for line in statement.splitlines() if line.strip()]\n",
        "\n",
        "        initial_line = \"\"\n",
        "        goal_line = \"\"\n",
        "        for line in lines:\n",
        "            lower = line.lower()\n",
        "            if lower.startswith(\"as initial conditions i have that\"):\n",
        "                initial_line = line\n",
        "            elif lower.startswith(\"my goal is to have that\"):\n",
        "                goal_line = line\n",
        "\n",
        "        if not initial_line or not goal_line:\n",
        "            return None\n",
        "\n",
        "        initial_text = re.sub(\n",
        "            r\"^as initial conditions i have that,?\\s*\",\n",
        "            \"\",\n",
        "            initial_line,\n",
        "            flags=re.IGNORECASE,\n",
        "        )\n",
        "        goal_text = re.sub(\n",
        "            r\"^my goal is to have that,?\\s*\",\n",
        "            \"\",\n",
        "            goal_line,\n",
        "            flags=re.IGNORECASE,\n",
        "        )\n",
        "\n",
        "        facts_initial = self._split_facts(initial_text)\n",
        "        facts_goal = self._split_facts(goal_text)\n",
        "\n",
        "        objects = set(re.findall(r\"object\\s+([a-z])\", statement.lower()))\n",
        "        state = {\n",
        "            \"harmony\": False,\n",
        "            \"planet\": set(),\n",
        "            \"province\": set(),\n",
        "            \"pain\": set(),\n",
        "            \"craves\": set(),\n",
        "        }\n",
        "        goals = {\"craves\": set()}\n",
        "\n",
        "        for fact in facts_initial:\n",
        "            m_crave = re.fullmatch(r\"object\\s+([a-z])\\s+craves\\s+object\\s+([a-z])\", fact)\n",
        "            if m_crave:\n",
        "                x, y = m_crave.group(1), m_crave.group(2)\n",
        "                state[\"craves\"].add((x, y))\n",
        "                objects.update([x, y])\n",
        "                continue\n",
        "\n",
        "            m_planet = re.fullmatch(r\"planet\\s+object\\s+([a-z])\", fact)\n",
        "            if m_planet:\n",
        "                x = m_planet.group(1)\n",
        "                state[\"planet\"].add(x)\n",
        "                objects.add(x)\n",
        "                continue\n",
        "\n",
        "            m_province = re.fullmatch(r\"province\\s+object\\s+([a-z])\", fact)\n",
        "            if m_province:\n",
        "                x = m_province.group(1)\n",
        "                state[\"province\"].add(x)\n",
        "                objects.add(x)\n",
        "                continue\n",
        "\n",
        "            if fact == \"harmony\":\n",
        "                state[\"harmony\"] = True\n",
        "\n",
        "        for fact in facts_goal:\n",
        "            m_goal = re.fullmatch(r\"object\\s+([a-z])\\s+craves\\s+object\\s+([a-z])\", fact)\n",
        "            if m_goal:\n",
        "                x, y = m_goal.group(1), m_goal.group(2)\n",
        "                goals[\"craves\"].add((x, y))\n",
        "                objects.update([x, y])\n",
        "\n",
        "        if not objects:\n",
        "            return None\n",
        "\n",
        "        return state, goals, sorted(objects)\n",
        "\n",
        "    def _state_key(self, state):\n",
        "        return (\n",
        "            state[\"harmony\"],\n",
        "            tuple(sorted(state[\"planet\"])),\n",
        "            tuple(sorted(state[\"province\"])),\n",
        "            tuple(sorted(state[\"pain\"])),\n",
        "            tuple(sorted(state[\"craves\"])),\n",
        "        )\n",
        "\n",
        "    def _goal_satisfied(self, state, goals) -> bool:\n",
        "        return goals[\"craves\"].issubset(state[\"craves\"])\n",
        "\n",
        "    def _copy_state(self, state):\n",
        "        return {\n",
        "            \"harmony\": state[\"harmony\"],\n",
        "            \"planet\": set(state[\"planet\"]),\n",
        "            \"province\": set(state[\"province\"]),\n",
        "            \"pain\": set(state[\"pain\"]),\n",
        "            \"craves\": set(state[\"craves\"]),\n",
        "        }\n",
        "\n",
        "    def _successors(self, state, objects):\n",
        "        ordered_actions = [\"attack\", \"succumb\", \"overcome\", \"feast\"]\n",
        "        result = []\n",
        "\n",
        "        for action in ordered_actions:\n",
        "            if action == \"attack\":\n",
        "                for x in objects:\n",
        "                    if state[\"harmony\"] and x in state[\"planet\"] and x in state[\"province\"]:\n",
        "                        nxt = self._copy_state(state)\n",
        "                        nxt[\"pain\"].add(x)\n",
        "                        nxt[\"province\"].discard(x)\n",
        "                        nxt[\"planet\"].discard(x)\n",
        "                        nxt[\"harmony\"] = False\n",
        "                        result.append((f\"(attack {x})\", nxt))\n",
        "\n",
        "            elif action == \"succumb\":\n",
        "                for x in objects:\n",
        "                    if x in state[\"pain\"]:\n",
        "                        nxt = self._copy_state(state)\n",
        "                        nxt[\"province\"].add(x)\n",
        "                        nxt[\"planet\"].add(x)\n",
        "                        nxt[\"harmony\"] = True\n",
        "                        nxt[\"pain\"].discard(x)\n",
        "                        result.append((f\"(succumb {x})\", nxt))\n",
        "\n",
        "            elif action == \"overcome\":\n",
        "                for x in objects:\n",
        "                    for y in objects:\n",
        "                        if x == y:\n",
        "                            continue\n",
        "                        if y in state[\"province\"] and x in state[\"pain\"]:\n",
        "                            nxt = self._copy_state(state)\n",
        "                            nxt[\"harmony\"] = True\n",
        "                            nxt[\"province\"].add(x)\n",
        "                            nxt[\"craves\"].add((x, y))\n",
        "                            nxt[\"province\"].discard(y)\n",
        "                            nxt[\"pain\"].discard(x)\n",
        "                            result.append((f\"(overcome {x} {y})\", nxt))\n",
        "\n",
        "            else:  # feast\n",
        "                for x in objects:\n",
        "                    for y in objects:\n",
        "                        if x == y:\n",
        "                            continue\n",
        "                        if state[\"harmony\"] and x in state[\"province\"] and (x, y) in state[\"craves\"]:\n",
        "                            nxt = self._copy_state(state)\n",
        "                            nxt[\"pain\"].add(x)\n",
        "                            nxt[\"province\"].add(y)\n",
        "                            nxt[\"craves\"].discard((x, y))\n",
        "                            nxt[\"province\"].discard(x)\n",
        "                            nxt[\"harmony\"] = False\n",
        "                            result.append((f\"(feast {x} {y})\", nxt))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _plan_with_bfs(self, scenario_context: str, max_depth: int = 14) -> list[str]:\n",
        "        parsed = self._parse_problem(scenario_context)\n",
        "        if parsed is None:\n",
        "            return []\n",
        "\n",
        "        initial_state, goals, objects = parsed\n",
        "        if self._goal_satisfied(initial_state, goals):\n",
        "            return []\n",
        "\n",
        "        queue = deque([(initial_state, [])])\n",
        "        visited = {self._state_key(initial_state): 0}\n",
        "\n",
        "        while queue:\n",
        "            state, plan = queue.popleft()\n",
        "            if len(plan) >= max_depth:\n",
        "                continue\n",
        "\n",
        "            for action_text, nxt in self._successors(state, objects):\n",
        "                new_plan = plan + [action_text]\n",
        "                key = self._state_key(nxt)\n",
        "                old_depth = visited.get(key)\n",
        "                if old_depth is not None and old_depth <= len(new_plan):\n",
        "                    continue\n",
        "                visited[key] = len(new_plan)\n",
        "\n",
        "                if self._goal_satisfied(nxt, goals):\n",
        "                    return new_plan\n",
        "\n",
        "                queue.append((nxt, new_plan))\n",
        "\n",
        "        return []\n",
        "\n",
        "    def _extract_actions(self, text: str) -> list[str]:\n",
        "        candidates = []\n",
        "        for line in text.splitlines():\n",
        "            normalized = line.strip().lower().replace(\"-\", \" \")\n",
        "            for pattern, formatter in self._patterns:\n",
        "                match = pattern.search(normalized)\n",
        "                if match:\n",
        "                    candidates.append(formatter(match))\n",
        "                    break\n",
        "\n",
        "        if candidates:\n",
        "            return self._trim_repeated_plan(candidates)\n",
        "\n",
        "        spans = []\n",
        "        lowered = text.lower().replace(\"-\", \" \")\n",
        "        for pattern, formatter in self._patterns:\n",
        "            for match in pattern.finditer(lowered):\n",
        "                spans.append((match.start(), formatter(match)))\n",
        "        spans.sort(key=lambda item: item[0])\n",
        "\n",
        "        return self._trim_repeated_plan([action for _, action in spans])\n",
        "\n",
        "    def _trim_repeated_plan(self, actions: list[str]) -> list[str]:\n",
        "        n = len(actions)\n",
        "        if n >= 2 and n % 2 == 0 and actions[: n // 2] == actions[n // 2 :]:\n",
        "            return actions[: n // 2]\n",
        "        return actions\n",
        "\n",
        "    def solve(self, scenario_context: str, llm_engine_func) -> list:\n",
        "        \"\"\"\n",
        "        Recibe el texto del escenario y la funcion del motor LLM.\n",
        "        Retorna una lista de acciones en formato canonico:\n",
        "        (attack x), (succumb x), (feast x y), (overcome x y)\n",
        "        \"\"\"\n",
        "        plan = self._plan_with_bfs(scenario_context)\n",
        "        if plan:\n",
        "            return plan\n",
        "\n",
        "        prompt_final = (\n",
        "            f\"{scenario_context}\\n\\n\"\n",
        "            \"Generate only the missing plan for the final [STATEMENT].\\n\"\n",
        "            \"Output rules:\\n\"\n",
        "            \"1) One action per line.\\n\"\n",
        "            \"2) Use ONLY: (attack x), (succumb x), (feast x y), (overcome x y).\\n\"\n",
        "            \"3) Use lowercase object symbols.\\n\"\n",
        "            \"4) Do not add explanations, numbering, or extra text.\"\n",
        "        )\n",
        "\n",
        "        raw = llm_engine_func(\n",
        "            prompt=prompt_final,\n",
        "            system=self.system_prompt,\n",
        "            max_new_tokens=220,\n",
        "        )\n",
        "        return self._extract_actions(raw)"
      ],
      "metadata": {
        "id": "zyZGNAXCspda"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dev Test"
      ],
      "metadata": {
        "id": "rbQZpHuDstwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "ARCHIVO_DESARROLLO = \"Examples.json\"\n",
        "\n",
        "def dev_test(n_casos=10):\n",
        "    print(f\"Cargando dataset de desarrollo: {ARCHIVO_DESARROLLO}\")\n",
        "    with open(ARCHIVO_DESARROLLO, 'r') as f:\n",
        "        casos = json.load(f)\n",
        "\n",
        "    agente = AssemblyAgent()\n",
        "    puntaje_total = 0.0\n",
        "    casos_evaluados = min(n_casos, len(casos)) # Limite para pruebas rapidas\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    for i in range(casos_evaluados):\n",
        "        caso = casos[i]\n",
        "        print(f\"Evaluando Tarea ID: {caso['assembly_task_id']} (Longitud optima: {caso['complexity_level']})\")\n",
        "\n",
        "        plan_generado = agente.solve(caso['scenario_context'], qwen)\n",
        "        plan_optimo = caso['target_action_sequence']\n",
        "\n",
        "        # Calculo de metrica\n",
        "        score = calcular_score_plan(plan_generado, plan_optimo)\n",
        "        puntaje_total += score\n",
        "\n",
        "        print(f\"Plan Generado: {plan_generado}\")\n",
        "        print(f\"Score obtenido: {score} / 10.0\\n\")\n",
        "\n",
        "    promedio = puntaje_total / casos_evaluados\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Puntaje Promedio en Desarrollo: {round(promedio, 2)} / 10.0\")\n",
        "\n",
        "dev_test(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i2FmpwFs1kU",
        "outputId": "2d3b7053-e90f-49ab-9989-96d51f50aad9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando dataset de desarrollo: Examples.json\n",
            "--------------------------------------------------\n",
            "Evaluando Tarea ID: task_6a4ed4586d (Longitud optima: 4)\n",
            "Plan Generado: ['(attack a)', '(overcome a c)', '(attack b)', '(overcome b a)']\n",
            "Score obtenido: 10.0 / 10.0\n",
            "\n",
            "Evaluando Tarea ID: task_b9a95877e2 (Longitud optima: 4)\n",
            "Plan Generado: ['(feast a b)', '(succumb a)', '(feast b c)', '(overcome b a)']\n",
            "Score obtenido: 10.0 / 10.0\n",
            "\n",
            "Evaluando Tarea ID: task_0d236ad4c6 (Longitud optima: 6)\n",
            "Plan Generado: ['(feast a c)', '(succumb a)', '(feast c b)', '(overcome c a)', '(attack b)', '(overcome b c)']\n",
            "Score obtenido: 10.0 / 10.0\n",
            "\n",
            "Evaluando Tarea ID: task_372c431054 (Longitud optima: 6)\n",
            "Plan Generado: ['(feast a c)', '(succumb a)', '(attack b)', '(overcome b c)', '(attack a)', '(overcome a b)']\n",
            "Score obtenido: 10.0 / 10.0\n",
            "\n",
            "Evaluando Tarea ID: task_79c0ae73f7 (Longitud optima: 2)\n",
            "Plan Generado: ['(attack a)', '(overcome a c)']\n",
            "Score obtenido: 10.0 / 10.0\n",
            "\n",
            "Evaluando Tarea ID: task_5cc798432f (Longitud optima: 4)\n",
            "Plan Generado: ['(feast a c)', '(succumb a)', '(attack c)', '(overcome c a)']\n",
            "Score obtenido: 10.0 / 10.0\n",
            "\n",
            "Evaluando Tarea ID: task_78ac2d8d7f (Longitud optima: 6)\n",
            "Plan Generado: ['(feast b c)', '(succumb b)', '(attack c)', '(overcome c b)', '(attack a)', '(overcome a c)']\n",
            "Score obtenido: 10.0 / 10.0\n",
            "\n",
            "Evaluando Tarea ID: task_9d7fd5f56c (Longitud optima: 6)\n",
            "Plan Generado: ['(feast b a)', '(succumb b)', '(attack a)', '(overcome a b)', '(attack c)', '(overcome c a)']\n",
            "Score obtenido: 10.0 / 10.0\n",
            "\n",
            "Evaluando Tarea ID: task_4371c597dd (Longitud optima: 6)\n",
            "Plan Generado: ['(feast b a)', '(succumb b)', '(attack c)', '(overcome c a)', '(attack b)', '(overcome b c)']\n",
            "Score obtenido: 10.0 / 10.0\n",
            "\n",
            "Evaluando Tarea ID: task_9aceb12382 (Longitud optima: 6)\n",
            "Plan Generado: ['(feast a c)', '(succumb a)', '(attack c)', '(overcome c b)', '(attack a)', '(overcome a c)']\n",
            "Score obtenido: 10.0 / 10.0\n",
            "\n",
            "--------------------------------------------------\n",
            "Puntaje Promedio en Desarrollo: 10.0 / 10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit"
      ],
      "metadata": {
        "id": "FritSRS1riej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "ARCHIVO_EVALUACION = \"Task.json\"\n",
        "ARCHIVO_SALIDA = \"submission.json\"\n",
        "\n",
        "def submit():\n",
        "    print(f\"Iniciando ejecucion sobre: {ARCHIVO_EVALUACION}\")\n",
        "    with open(ARCHIVO_EVALUACION, 'r') as f:\n",
        "        casos = json.load(f)\n",
        "\n",
        "    agente = AssemblyAgent()\n",
        "    resultados_entrega = []\n",
        "\n",
        "    for i, caso in enumerate(casos):\n",
        "        task_id = caso['assembly_task_id']\n",
        "        print(f\"Procesando caso {i+1}/{len(casos)} (ID: {task_id})...\")\n",
        "\n",
        "        try:\n",
        "            plan_generado = agente.solve(caso['scenario_context'], qwen)\n",
        "\n",
        "            resultados_entrega.append({\n",
        "                \"assembly_task_id\": task_id,\n",
        "                \"target_action_sequence\": plan_generado\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR critico en el caso {task_id}: {e}\")\n",
        "            print(\"Corrige tu codigo. https://www.youtube.com/watch?v=Y-U1calv6X8\")\n",
        "            return\n",
        "\n",
        "    # Guardar\n",
        "    with open(ARCHIVO_SALIDA, 'w') as f:\n",
        "        json.dump(resultados_entrega, f, indent=4)\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Exito. Archivo '{ARCHIVO_SALIDA}' generado correctamente.\")\n",
        "\n",
        "submit()"
      ],
      "metadata": {
        "id": "UfKnxnaYrdH5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85a341b0-11be-4d02-d6fa-9375d052e5dd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando ejecucion sobre: Task.json\n",
            "Procesando caso 1/50 (ID: task_f6c3f52f55)...\n",
            "Procesando caso 2/50 (ID: task_07a18910c7)...\n",
            "Procesando caso 3/50 (ID: task_cbe2649f6b)...\n",
            "Procesando caso 4/50 (ID: task_4f181b1e7e)...\n",
            "Procesando caso 5/50 (ID: task_9f39e7f413)...\n",
            "Procesando caso 6/50 (ID: task_a51e02706c)...\n",
            "Procesando caso 7/50 (ID: task_8a763f838b)...\n",
            "Procesando caso 8/50 (ID: task_4d92dfa4d1)...\n",
            "Procesando caso 9/50 (ID: task_19c005f4fa)...\n",
            "Procesando caso 10/50 (ID: task_98c536f455)...\n",
            "Procesando caso 11/50 (ID: task_b516b29d5b)...\n",
            "Procesando caso 12/50 (ID: task_2833f3d973)...\n",
            "Procesando caso 13/50 (ID: task_a2e1da433c)...\n",
            "Procesando caso 14/50 (ID: task_b05ad833d2)...\n",
            "Procesando caso 15/50 (ID: task_a90c8a54bb)...\n",
            "Procesando caso 16/50 (ID: task_dae8652991)...\n",
            "Procesando caso 17/50 (ID: task_07201ae0aa)...\n",
            "Procesando caso 18/50 (ID: task_37dd853373)...\n",
            "Procesando caso 19/50 (ID: task_784946134b)...\n",
            "Procesando caso 20/50 (ID: task_32d7e53ce2)...\n",
            "Procesando caso 21/50 (ID: task_9b148ab9d7)...\n",
            "Procesando caso 22/50 (ID: task_6511a2f246)...\n",
            "Procesando caso 23/50 (ID: task_2cdb57bdd6)...\n",
            "Procesando caso 24/50 (ID: task_d9cb8a7219)...\n",
            "Procesando caso 25/50 (ID: task_e8b4a49b09)...\n",
            "Procesando caso 26/50 (ID: task_380244626c)...\n",
            "Procesando caso 27/50 (ID: task_0c4a4b37d9)...\n",
            "Procesando caso 28/50 (ID: task_7e684c4e53)...\n",
            "Procesando caso 29/50 (ID: task_251314c577)...\n",
            "Procesando caso 30/50 (ID: task_c24d1608bd)...\n",
            "Procesando caso 31/50 (ID: task_2661dd18d3)...\n",
            "Procesando caso 32/50 (ID: task_a0a699d182)...\n",
            "Procesando caso 33/50 (ID: task_4df345721b)...\n",
            "Procesando caso 34/50 (ID: task_6d18e048c6)...\n",
            "Procesando caso 35/50 (ID: task_bd47b6bc0e)...\n",
            "Procesando caso 36/50 (ID: task_faa891c9fb)...\n",
            "Procesando caso 37/50 (ID: task_9767ecb84d)...\n",
            "Procesando caso 38/50 (ID: task_382b33a282)...\n",
            "Procesando caso 39/50 (ID: task_8d7fc8623b)...\n",
            "Procesando caso 40/50 (ID: task_0eb88038e5)...\n",
            "Procesando caso 41/50 (ID: task_d3fb474049)...\n",
            "Procesando caso 42/50 (ID: task_7d66e7541e)...\n",
            "Procesando caso 43/50 (ID: task_7564916d70)...\n",
            "Procesando caso 44/50 (ID: task_60d8fa6441)...\n",
            "Procesando caso 45/50 (ID: task_7c8f882efa)...\n",
            "Procesando caso 46/50 (ID: task_182c209cb4)...\n",
            "Procesando caso 47/50 (ID: task_8631399361)...\n",
            "Procesando caso 48/50 (ID: task_ff22fcbdb7)...\n",
            "Procesando caso 49/50 (ID: task_9091b8dd04)...\n",
            "Procesando caso 50/50 (ID: task_fd0b531f2d)...\n",
            "--------------------------------------------------\n",
            "Exito. Archivo 'submission.json' generado correctamente.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "61284b4523404474bf6a6a65fedaf8f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_288b63bbdd23400e8f1114d1335fa707",
              "IPY_MODEL_7ff0a368d6564b4ca93f2a1e04fd8656",
              "IPY_MODEL_494827445a8343859d82714dfe5bdea6"
            ],
            "layout": "IPY_MODEL_88419be370434fcb91c9871dd65184ec"
          }
        },
        "288b63bbdd23400e8f1114d1335fa707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1302a0ff92124f80964f2898444cf336",
            "placeholder": "​",
            "style": "IPY_MODEL_95c31a75b7f541ec86386cdda63a70ac",
            "value": "Loading weights: 100%"
          }
        },
        "7ff0a368d6564b4ca93f2a1e04fd8656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd08466289694241a448ec5db68ed70d",
            "max": 399,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d4578f6549f45e2ac6020e1af149fe5",
            "value": 399
          }
        },
        "494827445a8343859d82714dfe5bdea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e78fa926798941468620aaf74461c8c6",
            "placeholder": "​",
            "style": "IPY_MODEL_6e7eadc1f77646a6a1bed5e2230871cc",
            "value": " 399/399 [01:14&lt;00:00,  6.11it/s, Materializing param=model.norm.weight]"
          }
        },
        "88419be370434fcb91c9871dd65184ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1302a0ff92124f80964f2898444cf336": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95c31a75b7f541ec86386cdda63a70ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd08466289694241a448ec5db68ed70d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d4578f6549f45e2ac6020e1af149fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e78fa926798941468620aaf74461c8c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e7eadc1f77646a6a1bed5e2230871cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}